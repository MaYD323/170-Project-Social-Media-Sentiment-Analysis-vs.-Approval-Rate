{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from gensim.models import word2vec\n",
    "anger = pd.read_csv('/Users/yindima/Desktop/170 project/Train dataset/anger-ratings-0to1.train.txt', sep = '\\t', names =  ['id','text','emotion','intensity'])\n",
    "fear = pd.read_csv('/Users/yindima//Desktop/170 project/Train dataset/fear-ratings-0to1.train.txt', sep = '\\t', names =  ['id','text','emotion','intensity'])\n",
    "joy = pd.read_csv('/Users/yindima//Desktop/170 project/Train dataset/joy-ratings-0to1.train.txt', sep = '\\t', names =  ['id','text','emotion','intensity'])\n",
    "sadness = pd.read_csv('/Users/yindima//Desktop/170 project/Train dataset/sadness-ratings-0to1.train.txt', sep = '\\t', names =  ['id','text','emotion','intensity'])\n",
    "\n",
    "data = pd.concat([anger, fear, joy, sadness],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our labeled twitter data is from http://saifmohammad.com/WebPages/EmotionIntensity-SharedTask.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data.drop(['id','intensity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How the fu*k! Who the heck! moved my fridge!.....</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So my Indian Uber driver just called someone t...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DPD_UK I asked for my parcel to be delivered ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>so ef whichever butt wipe pulled the fire alar...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Don't join @BTCare they put the phone down on ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>My blood is boiling</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>When you've still got a whole season of Wentwo...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@bt_uk why does tracking show my equipment del...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@TeamShanny legit why i am so furious with him...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How is it suppose to work if you do that? Wtf ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text emotion\n",
       "0  How the fu*k! Who the heck! moved my fridge!.....   anger\n",
       "1  So my Indian Uber driver just called someone t...   anger\n",
       "2  @DPD_UK I asked for my parcel to be delivered ...   anger\n",
       "3  so ef whichever butt wipe pulled the fire alar...   anger\n",
       "4  Don't join @BTCare they put the phone down on ...   anger\n",
       "5                                My blood is boiling   anger\n",
       "6  When you've still got a whole season of Wentwo...   anger\n",
       "7  @bt_uk why does tracking show my equipment del...   anger\n",
       "8  @TeamShanny legit why i am so furious with him...   anger\n",
       "9  How is it suppose to work if you do that? Wtf ...   anger"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n",
    "## uniformity\n",
    "We firstly need to bring the words into uniform, so we would need to make every word lowercase and remove punctuation and stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making all letters lowercase\n",
    "data_cleaned['text'] = data_cleaned['text'].apply(lambda x: x.lower())\n",
    "#Removing Punctuation, Symbols\n",
    "data_cleaned['text'] = data_cleaned['text'].str.replace('[^\\w\\s]',' ')\n",
    "#Removing Stop Words\n",
    "stop_words = stopwords.words('english')\n",
    "for i in range(len(data_cleaned)):\n",
    "    text = ''\n",
    "    for word in data_cleaned.loc[i]['text'].split():\n",
    "        if word not in stop_words:\n",
    "            text+=' '+word\n",
    "    data_cleaned.loc[i]['text'] = text.strip(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fu k heck moved fridge knock landlord door ang...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>indian uber driver called someone n word movin...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dpd_uk asked parcel delivered pick store addre...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ef whichever butt wipe pulled fire alarm davis...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>join btcare put phone talk rude taking money a...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>blood boiling</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>still got whole season wentworth watch stupid ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bt_uk tracking show equipment delivered servic...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>teamshanny legit furious people fucking idiots</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>suppose work wtf dude thanks pissing</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text emotion\n",
       "0  fu k heck moved fridge knock landlord door ang...   anger\n",
       "1  indian uber driver called someone n word movin...   anger\n",
       "2  dpd_uk asked parcel delivered pick store addre...   anger\n",
       "3  ef whichever butt wipe pulled fire alarm davis...   anger\n",
       "4  join btcare put phone talk rude taking money a...   anger\n",
       "5                                      blood boiling   anger\n",
       "6  still got whole season wentworth watch stupid ...   anger\n",
       "7  bt_uk tracking show equipment delivered servic...   anger\n",
       "8     teamshanny legit furious people fucking idiots   anger\n",
       "9               suppose work wtf dude thanks pissing   anger"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation\n",
    "we will need to return the words into their root "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(data_cleaned)):\n",
    "    text = ''\n",
    "    for word in data_cleaned.loc[i]['text'].split():\n",
    "        text+=' '+lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word),pos = 'a'), pos = 'v')\n",
    "    data_cleaned.loc[i]['text'] = text.strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fu k heck move fridge knock landlord door angr...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>indian uber driver call someone n word move ve...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dpd_uk ask parcel deliver pick store address f...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ef whichever butt wipe pull fire alarm davis b...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>join btcare put phone talk rude take money acc...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>blood boil</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>still get whole season wentworth watch stupid ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bt_uk track show equipment deliver service sud...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>teamshanny legit furious people fuck idiot</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>suppose work wtf dude thank piss</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text emotion\n",
       "0  fu k heck move fridge knock landlord door angr...   anger\n",
       "1  indian uber driver call someone n word move ve...   anger\n",
       "2  dpd_uk ask parcel deliver pick store address f...   anger\n",
       "3  ef whichever butt wipe pull fire alarm davis b...   anger\n",
       "4  join btcare put phone talk rude take money acc...   anger\n",
       "5                                         blood boil   anger\n",
       "6  still get whole season wentworth watch stupid ...   anger\n",
       "7  bt_uk track show equipment deliver service sud...   anger\n",
       "8         teamshanny legit furious people fuck idiot   anger\n",
       "9                   suppose work wtf dude thank piss   anger"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing rare words\n",
    "firstly get the rarest 5,000 words, since they may not valueable for being feature, so we will remove them from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "for i in range(len(data_cleaned)):\n",
    "    for word in data_cleaned.loc[i]['text'].split():\n",
    "        if word in word_dict:\n",
    "            word_dict[word]+=1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "rarest_words = sorted(list(word_dict.keys()), key = lambda x:word_dict[x])[:5000]\n",
    "\n",
    "for i in range(len(data_cleaned)):\n",
    "    text = ''\n",
    "    for word in data_cleaned.loc[i]['text'].split():\n",
    "        if word not in rarest_words:\n",
    "            text+=' '+word\n",
    "    data_cleaned.loc[i]['text'] = text.strip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>move knock door angry mad</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>indian uber driver call someone n word move ve...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ask deliver pick store address fume</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whichever butt wipe pull fire alarm bc sound p...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>join put phone talk rude take money acc fume</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>blood boil</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>still get whole season watch stupid cunt work ...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>track show equipment deliver service already 3...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>legit furious people fuck idiot</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>suppose work wtf dude thank piss</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text emotion\n",
       "0                          move knock door angry mad   anger\n",
       "1  indian uber driver call someone n word move ve...   anger\n",
       "2                ask deliver pick store address fume   anger\n",
       "3  whichever butt wipe pull fire alarm bc sound p...   anger\n",
       "4       join put phone talk rude take money acc fume   anger\n",
       "5                                         blood boil   anger\n",
       "6  still get whole season watch stupid cunt work ...   anger\n",
       "7  track show equipment deliver service already 3...   anger\n",
       "8                    legit furious people fuck idiot   anger\n",
       "9                   suppose work wtf dude thank piss   anger"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start from simplest model, only care about anger and joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_and_joy_cleaned = pd.concat([data_cleaned[data_cleaned['emotion'] == 'joy'],data_cleaned[data_cleaned['emotion'] == 'anger']], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>get back see garydelaney burslem amaze face st...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oh dear even absolute hilarity think laugh muc...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wait week game cheer friday</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank much sweet thoughtful make day joyful love</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feel bless work family nothing love amp make s...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text emotion\n",
       "0  get back see garydelaney burslem amaze face st...     joy\n",
       "1  oh dear even absolute hilarity think laugh muc...     joy\n",
       "2                        wait week game cheer friday     joy\n",
       "3   thank much sweet thoughtful make day joyful love     joy\n",
       "4  feel bless work family nothing love amp make s...     joy"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anger_and_joy_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "#### We will try 2 different feature extraction, TF-IDF and count vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "#This parameter gives the relative importance of a term in the data \n",
    "#and is a measure of how frequently and rarely it appears in the text. \n",
    "#This can be directly extracted in python\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(anger_and_joy_cleaned.emotion.values)\n",
    "X_train, X_val, y_train, y_val = train_test_split(anger_and_joy_cleaned.text.values, y, stratify=y, random_state=23, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=1000, analyzer='word',ngram_range=(1,3))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes tfidf accuracy 0.4226190476190476\n",
      "svm using tfidf accuracy 0.5119047619047619\n",
      "log reg tfidf accuracy 0.4583333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yindima/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest tfidf accuracy 0.44642857142857145\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Multinomial Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "y_pred = nb.predict(X_val_tfidf)\n",
    "print(\"naive bayes tfidf accuracy %s\" %  accuracy_score(y_pred, y_val))\n",
    "# Model 2: Linear SVM\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
    "lsvm.fit(X_train_tfidf, y_train)\n",
    "y_pred = lsvm.predict(X_val_tfidf)\n",
    "print('svm using tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "# Model 3: logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1)\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "y_pred = logreg.predict(X_val_tfidf)\n",
    "print('log reg tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "# Model 4: Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "y_pred = rf.predict(X_val_tfidf)\n",
    "print('random forest tfidf accuracy %s' % accuracy_score(y_pred, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count vector\n",
    "#This is another feature we consider and as the name suggests we transform our tweet into an\n",
    "#array having the count of appearances of each word in it. \n",
    "#The intuition here is that the text that conveys similar emotions may have the same words repeated over \n",
    "#and over again. This is more like the direct approach.\n",
    "count_vect = CountVectorizer(analyzer='word',ngram_range = (1,2))\n",
    "count_vect.fit(anger_and_joy_cleaned['text'])\n",
    "X_train_count =  count_vect.transform(X_train)\n",
    "X_val_count =  count_vect.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes count vectors accuracy 0.9404761904761905\n",
      "lsvm using count vectors accuracy 0.9523809523809523\n",
      "log reg count vectors accuracy 0.9345238095238095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yindima/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest with count vectors accuracy 0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Multinomial Naive Bayes Classifier\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_count, y_train)\n",
    "y_pred = nb.predict(X_val_count)\n",
    "print('naive bayes count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "# Model 2: Linear SVM\n",
    "\n",
    "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
    "lsvm.fit(X_train_count, y_train)\n",
    "y_pred = lsvm.predict(X_val_count)\n",
    "print('lsvm using count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "# Model 3: Logistic Regression\n",
    "\n",
    "logreg = LogisticRegression(C=1)\n",
    "logreg.fit(X_train_count, y_train)\n",
    "y_pred = logreg.predict(X_val_count)\n",
    "print('log reg count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "# Model 4: Random Forest Classifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(X_train_count, y_train)\n",
    "y_pred = rf.predict(X_val_count)\n",
    "print('random forest with count vectors accuracy %s' % accuracy_score(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try on 4 classes with count vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classes_': array(['anger', 'fear', 'joy', 'sadness'], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(data_cleaned.emotion.values)\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_cleaned.text.values, y, stratify=y, random_state=23, test_size=0.1, shuffle=True)\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word',ngram_range = (1,2))\n",
    "count_vect.fit(data_cleaned['text'])\n",
    "X_train_count =  count_vect.transform(X_train)\n",
    "X_val_count =  count_vect.transform(X_val)\n",
    "\n",
    "print(lbl_enc.__dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes count vectors accuracy 0.8480662983425414\n",
      "lsvm using count vectors accuracy 0.9005524861878453\n",
      "log reg count vectors accuracy 0.8784530386740331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yindima/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/yindima/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest with count vectors accuracy 0.8453038674033149\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Multinomial Naive Bayes Classifier\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_count, y_train)\n",
    "y_pred = nb.predict(X_val_count)\n",
    "print('naive bayes count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "# Model 2: Linear SVM\n",
    "\n",
    "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
    "lsvm.fit(X_train_count, y_train)\n",
    "y_pred = lsvm.predict(X_val_count)\n",
    "print('lsvm using count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "# Model 3: Logistic Regression\n",
    "\n",
    "logreg = LogisticRegression(C=1)\n",
    "logreg.fit(X_train_count, y_train)\n",
    "y_pred = logreg.predict(X_val_count)\n",
    "print('log reg count vectors accuracy %s' % accuracy_score(y_pred, y_val))\n",
    "\n",
    "# Model 4: Random Forest Classifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(X_train_count, y_train)\n",
    "y_pred = rf.predict(X_val_count)\n",
    "print('random forest with count vectors accuracy %s' % accuracy_score(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try on more test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_test = pd.read_csv('/Users/yindima//Desktop/170 project/Test dataset/anger-ratings-0to1.test.gold.txt', sep = '\\t', names =  ['id','text','emotion','intensity'])\n",
    "fear_test = pd.read_csv('/Users/yindima//Desktop/170 project/Test dataset/fear-ratings-0to1.test.gold.txt', sep = '\\t', names =  ['id','text','emotion','intensity'])\n",
    "joy_test = pd.read_csv('/Users/yindima//Desktop/170 project/Test dataset/joy-ratings-0to1.test.gold.txt', sep = '\\t', names =  ['id','text','emotion','intensity'])\n",
    "sadness_test = pd.read_csv('/Users/yindima//Desktop/170 project/Test dataset/sadness-ratings-0to1.test.gold.txt', sep = '\\t', names =  ['id','text','emotion','intensity'])\n",
    "\n",
    "test_data = pd.concat([anger_test, fear_test, joy_test, sadness_test],ignore_index=True)\n",
    "test_data_cleaned = test_data.drop(['id','intensity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unification(data_cleaned):\n",
    "    #Making all letters lowercase\n",
    "    data_cleaned['text'] = data_cleaned['text'].apply(lambda x: x.lower())\n",
    "    #Removing Punctuation, Symbols\n",
    "    data_cleaned['text'] = data_cleaned['text'].str.replace('[^\\w\\s]',' ')\n",
    "    #Removing Stop Words\n",
    "    stop_words = stopwords.words('english')\n",
    "    for i in range(len(data_cleaned)):\n",
    "        text = ''\n",
    "        for word in data_cleaned.loc[i]['text'].split():\n",
    "            if word not in stop_words:\n",
    "                text+=' '+word\n",
    "        data_cleaned.loc[i]['text'] = text.strip(' ')\n",
    "    return data_cleaned\n",
    "\n",
    "def lemmatisation(data_cleaned):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(data_cleaned)):\n",
    "        text = ''\n",
    "        for word in data_cleaned.loc[i]['text'].split():\n",
    "            text+=' '+lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word),pos = 'a'), pos = 'v')\n",
    "        data_cleaned.loc[i]['text'] = text.strip(' ')\n",
    "    return data_cleaned\n",
    "\n",
    "def removing_rare_words(data_cleaned, num_to_ignore = 5000):\n",
    "    word_dict = {}\n",
    "    for i in range(len(data_cleaned)):\n",
    "        for word in data_cleaned.loc[i]['text'].split():\n",
    "            if word in word_dict:\n",
    "                word_dict[word]+=1\n",
    "            else:\n",
    "                word_dict[word] = 1\n",
    "    rarest_words = sorted(list(word_dict.keys()), key = lambda x:word_dict[x])[:num_to_ignore]\n",
    "\n",
    "    for i in range(len(data_cleaned)):\n",
    "        text = ''\n",
    "        for word in data_cleaned.loc[i]['text'].split():\n",
    "            if word not in rarest_words:\n",
    "                text+=' '+word\n",
    "        data_cleaned.loc[i]['text'] = text.strip(' ')\n",
    "    return data_cleaned\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_cleaned = removing_rare_words(lemmatisation(unification(test_data_cleaned)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>point today someone say something kind burst eye</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>game day minus 14 30 relentless</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>game piss game year blood boil time turn</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>find candice candace pout like</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>come mum 25k tweet</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text emotion\n",
       "0  point today someone say something kind burst eye   anger\n",
       "1                   game day minus 14 30 relentless   anger\n",
       "2          game piss game year blood boil time turn   anger\n",
       "3                    find candice candace pout like   anger\n",
       "4                                come mum 25k tweet   anger"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y_test = lbl_enc.fit_transform(test_data_cleaned.emotion.values)\n",
    "x_test = test_data_cleaned['text']\n",
    "x_test_count =  count_vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes count vectors accuracy 0.746658179503501\n",
      "lsvm using count vectors accuracy 0.821769573520051\n",
      "log reg count vectors accuracy 0.8128580521960534\n",
      "random forest with count vectors accuracy 0.8157224697644813\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Multinomial Naive Bayes Classifier\n",
    "\n",
    "\n",
    "y_pred = nb.predict(x_test_count)\n",
    "print('naive bayes count vectors accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "\n",
    "# Model 2: Linear SVM\n",
    "\n",
    "\n",
    "y_pred = lsvm.predict(x_test_count)\n",
    "print('lsvm using count vectors accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "\n",
    "# Model 3: Logistic Regression\n",
    "\n",
    "\n",
    "y_pred = logreg.predict(x_test_count)\n",
    "print('log reg count vectors accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "\n",
    "# Model 4: Random Forest Classifier\n",
    "\n",
    "\n",
    "y_pred = rf.predict(x_test_count)\n",
    "print('random forest with count vectors accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try on dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_dev = pd.read_csv('/Users/yindima//Desktop/170 project/Dev dataset/anger-ratings-0to1.dev.gold.txt', sep = '\\t', names =  ['id','text','emotion','intensity'])\n",
    "fear_dev = pd.read_csv('/Users/yindima//Desktop/170 project/Dev dataset/fear-ratings-0to1.dev.gold.txt', sep = '\\t', names =  ['id','text','emotion','intensity'])\n",
    "joy_dev = pd.read_csv('/Users/yindima//Desktop/170 project/Dev dataset/joy-ratings-0to1.dev.gold.txt', sep = '\\t', names =  ['id','text','emotion','intensity'])\n",
    "sadness_dev = pd.read_csv('/Users/yindima//Desktop/170 project/Dev dataset/sadness-ratings-0to1.dev.gold.txt', sep = '\\t', names =  ['id','text','emotion','intensity'])\n",
    "\n",
    "dev_data = pd.concat([anger_dev, fear_dev, joy_dev, sadness_dev],ignore_index=True)\n",
    "dev_data_cleaned = dev_data.drop(['id','intensity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dont insult</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>would take offense actually snap</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>game affront god man must never speak</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ask start rag call</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sometimes get mad something minuscule try ruin...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text emotion\n",
       "0                                        dont insult   anger\n",
       "1                   would take offense actually snap   anger\n",
       "2              game affront god man must never speak   anger\n",
       "3                                 ask start rag call   anger\n",
       "4  sometimes get mad something minuscule try ruin...   anger"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data_cleaned = removing_rare_words(lemmatisation(unification(dev_data_cleaned)),1000)\n",
    "dev_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y_dev = lbl_enc.fit_transform(dev_data_cleaned.emotion.values)\n",
    "x_dev = dev_data_cleaned['text']\n",
    "x_dev_count =  count_vect.transform(x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes count vectors accuracy 0.7463976945244957\n",
      "lsvm using count vectors accuracy 0.7982708933717579\n",
      "log reg count vectors accuracy 0.7896253602305475\n",
      "random forest with count vectors accuracy 0.7665706051873199\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Multinomial Naive Bayes Classifier\n",
    "\n",
    "\n",
    "y_pred = nb.predict(x_dev_count)\n",
    "print('naive bayes count vectors accuracy %s' % accuracy_score(y_pred, y_dev))\n",
    "\n",
    "# Model 2: Linear SVM\n",
    "\n",
    "\n",
    "y_pred = lsvm.predict(x_dev_count)\n",
    "print('lsvm using count vectors accuracy %s' % accuracy_score(y_pred, y_dev))\n",
    "\n",
    "# Model 3: Logistic Regression\n",
    "\n",
    "\n",
    "y_pred = logreg.predict(x_dev_count)\n",
    "print('log reg count vectors accuracy %s' % accuracy_score(y_pred, y_dev))\n",
    "\n",
    "# Model 4: Random Forest Classifier\n",
    "\n",
    "\n",
    "y_pred = rf.predict(x_dev_count)\n",
    "print('random forest with count vectors accuracy %s' % accuracy_score(y_pred, y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "pickle.dump(logreg, open('/Users/yindima/Desktop/170 project/emotion_model/logreg_model', 'wb'))\n",
    "# save the model to disk\n",
    "pickle.dump(lsvm, open('/Users/yindima/Desktop/170 project/emotion_model/lsvm_model', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try other modern word embedding tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>seg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[tensor(-0.0974, grad_fn=&lt;SelectBackward&gt;), te...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[tensor(-0.1502, grad_fn=&lt;SelectBackward&gt;), te...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tensor(0.1921, grad_fn=&lt;SelectBackward&gt;), ten...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[tensor(0.1514, grad_fn=&lt;SelectBackward&gt;), ten...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tensor(0.0650, grad_fn=&lt;SelectBackward&gt;), ten...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text emotion  \\\n",
       "0  [tensor(-0.0974, grad_fn=<SelectBackward>), te...   anger   \n",
       "1  [tensor(-0.1502, grad_fn=<SelectBackward>), te...   anger   \n",
       "2  [tensor(0.1921, grad_fn=<SelectBackward>), ten...   anger   \n",
       "3  [tensor(0.1514, grad_fn=<SelectBackward>), ten...   anger   \n",
       "4  [tensor(0.0650, grad_fn=<SelectBackward>), ten...   anger   \n",
       "\n",
       "                                                seg  \n",
       "0                                   [1, 1, 1, 1, 1]  \n",
       "1                 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "2                             [1, 1, 1, 1, 1, 1, 1]  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "4                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n",
    "data_cleaned_index = data_cleaned.copy()\n",
    "data_cleaned_index['seg'] = data_cleaned_index['text']\n",
    "for i in range(len(data_cleaned_index)):\n",
    "    data_cleaned_index.loc[i]['text']  = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(data_cleaned_index.loc[i]['text']))\n",
    "    data_cleaned_index.loc[i]['seg'] = [1]*len(data_cleaned_index.loc[i]['text'])\n",
    "   \n",
    "    tokens_tensor = torch.tensor([data_cleaned_index.loc[i]['text']])\n",
    "    segments_tensors = torch.tensor([data_cleaned_index.loc[i]['seg']])\n",
    "    \n",
    "    \n",
    "    encoded_layers, x = model(tokens_tensor, segments_tensors)\n",
    "    \n",
    "    token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    \n",
    "    token_vecs = encoded_layers[11][0]\n",
    "    # Calculate the average of all 22 token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    data_cleaned_index.loc[i]['text'] = sentence_embedding\n",
    "    del tokens_tensor,segments_tensors,encoded_layers, x,token_embeddings,token_vecs,sentence_embedding\n",
    "data_cleaned_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_cleaned_index)):\n",
    "    data_cleaned_index.loc[i]['text'] = data_cleaned_index.loc[i]['text'].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>seg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.09739778, 0.6434248, -0.30084196, -0.60367...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.15020666, -0.025147611, 0.58002543, -0.028...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.19213702, 0.16715051, 0.5674692, -0.3713170...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.15143839, 0.3220565, 0.3240387, -0.00035042...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.06495898, -0.0065595717, 0.5851037, 0.08516...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text emotion  \\\n",
       "0  [-0.09739778, 0.6434248, -0.30084196, -0.60367...   anger   \n",
       "1  [-0.15020666, -0.025147611, 0.58002543, -0.028...   anger   \n",
       "2  [0.19213702, 0.16715051, 0.5674692, -0.3713170...   anger   \n",
       "3  [0.15143839, 0.3220565, 0.3240387, -0.00035042...   anger   \n",
       "4  [0.06495898, -0.0065595717, 0.5851037, 0.08516...   anger   \n",
       "\n",
       "                                                seg  \n",
       "0                                   [1, 1, 1, 1, 1]  \n",
       "1                 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "2                             [1, 1, 1, 1, 1, 1, 1]  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "4                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned_index_c = data_cleaned_index.copy()\n",
    "for i in range(len(data_cleaned_index_c)):\n",
    "    data_cleaned_index_c.loc[i]['text'] = list(data_cleaned_index_c.loc[i]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "y = lbl_enc.fit_transform(data_cleaned_index.emotion.values)\n",
    "X_train_bert, X_val_bert, y_train_bert, y_val_bert = train_test_split(data_cleaned_index.text.values, y, stratify=y, random_state=23, test_size=0.1, shuffle=True)\n",
    "scaler.fit(list(data_cleaned_index.text.values))\n",
    "X_train_bert = scaler.transform(list(X_train_bert))\n",
    "X_val_bert = scaler.transform(list(X_val_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes BERT accuracy 0.39502762430939226\n",
      "svm using BERT accuracy 0.5524861878453039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yindima/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/yindima/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log reg BERT accuracy 0.5939226519337016\n",
      "random forest BERT accuracy 0.5027624309392266\n"
     ]
    }
   ],
   "source": [
    "##### Model 1: Multinomial Naive Bayes Classifier\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(list(X_train_bert), y_train_bert)\n",
    "y_pred = nb.predict(X_val_bert)\n",
    "print(\"naive bayes BERT accuracy %s\" %  accuracy_score(y_pred, y_val_bert))\n",
    "# Model 2: Linear SVM\n",
    "\n",
    "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
    "lsvm.fit(X_train_bert, y_train_bert)\n",
    "y_pred = lsvm.predict(X_val_bert)\n",
    "print('svm using BERT accuracy %s' % accuracy_score(y_pred, y_val_bert))\n",
    "\n",
    "# Model 3: logistic regression\n",
    "\n",
    "logreg = LogisticRegression(C=1)\n",
    "logreg.fit(X_train_bert, y_train_bert)\n",
    "y_pred = logreg.predict(X_val_bert)\n",
    "print('log reg BERT accuracy %s' % accuracy_score(y_pred, y_val_bert))\n",
    "\n",
    "# Model 4: Random Forest Classifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(X_train_bert, y_train_bert)\n",
    "y_pred = rf.predict(X_val_bert)\n",
    "print('random forest BERT accuracy %s' % accuracy_score(y_pred, y_val_bert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify relevant and irrelevant tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>text</th>\n",
       "      <th>d</th>\n",
       "      <th>lb</th>\n",
       "      <th>r</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.242949e+18</td>\n",
       "      <td>@instinctnaturel @TJCrick4 @dilenz2 @rachs80 @...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.242718e+18</td>\n",
       "      <td>RT @RebekahsRight: \"The SARS-CoV-2 coronavirus...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.242828e+18</td>\n",
       "      <td>@realDonaldTrump Ozone is one the most powerfu...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.242881e+18</td>\n",
       "      <td>RT @OccupyOneLove: @LillianOrlando4 @oveoblu ....</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.242893e+18</td>\n",
       "      <td>@ScottyDdoogie @unperturbable @RepAdamSchiff W...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tweetid                                               text  \\\n",
       "0  1.242949e+18  @instinctnaturel @TJCrick4 @dilenz2 @rachs80 @...   \n",
       "1  1.242718e+18  RT @RebekahsRight: \"The SARS-CoV-2 coronavirus...   \n",
       "2  1.242828e+18  @realDonaldTrump Ozone is one the most powerfu...   \n",
       "3  1.242881e+18  RT @OccupyOneLove: @LillianOrlando4 @oveoblu ....   \n",
       "4  1.242893e+18  @ScottyDdoogie @unperturbable @RepAdamSchiff W...   \n",
       "\n",
       "            d   lb    r  Unnamed: 5  Unnamed: 6  \n",
       "0  2020-03-25  NaN  0.0         NaN         NaN  \n",
       "1  2020-03-25  4.0  1.0         NaN         NaN  \n",
       "2  2020-03-25  1.0  0.0         NaN         NaN  \n",
       "3  2020-03-25  4.0  0.0         NaN         NaN  \n",
       "4  2020-03-25  2.0  1.0         NaN         NaN  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant = pd.read_csv('/Users/yindima/Desktop/170 project/relevant.csv')\n",
    "relevant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>text</th>\n",
       "      <th>d</th>\n",
       "      <th>lb</th>\n",
       "      <th>r</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.242949e+18</td>\n",
       "      <td>@instinctnaturel @TJCrick4 @dilenz2 @rachs80 @...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.242718e+18</td>\n",
       "      <td>RT @RebekahsRight: \"The SARS-CoV-2 coronavirus...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.242828e+18</td>\n",
       "      <td>@realDonaldTrump Ozone is one the most powerfu...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.242881e+18</td>\n",
       "      <td>RT @OccupyOneLove: @LillianOrlando4 @oveoblu ....</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.242893e+18</td>\n",
       "      <td>@ScottyDdoogie @unperturbable @RepAdamSchiff W...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       tweetid                                               text  \\\n",
       "0      0  1.242949e+18  @instinctnaturel @TJCrick4 @dilenz2 @rachs80 @...   \n",
       "1      1  1.242718e+18  RT @RebekahsRight: \"The SARS-CoV-2 coronavirus...   \n",
       "2      2  1.242828e+18  @realDonaldTrump Ozone is one the most powerfu...   \n",
       "3      3  1.242881e+18  RT @OccupyOneLove: @LillianOrlando4 @oveoblu ....   \n",
       "4      4  1.242893e+18  @ScottyDdoogie @unperturbable @RepAdamSchiff W...   \n",
       "\n",
       "            d   lb    r  Unnamed: 5  Unnamed: 6  \n",
       "0  2020-03-25  NaN  0.0         NaN         NaN  \n",
       "1  2020-03-25  4.0  1.0         NaN         NaN  \n",
       "2  2020-03-25  1.0  0.0         NaN         NaN  \n",
       "3  2020-03-25  4.0  0.0         NaN         NaN  \n",
       "4  2020-03-25  2.0  1.0         NaN         NaN  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_r = relevant[pd.notna(relevant['r'])].reset_index()\n",
    "with_r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yindima/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/Users/yindima/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/yindima/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "relevant_cleaned = removing_rare_words(lemmatisation(unification(with_r)),10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>text</th>\n",
       "      <th>d</th>\n",
       "      <th>lb</th>\n",
       "      <th>r</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.242949e+18</td>\n",
       "      <td>instinctnaturel  tjcrick4  dilenz2  rachs80  ...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.242718e+18</td>\n",
       "      <td>rt  rebekahsright   the sars cov 2 coronavirus...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.242828e+18</td>\n",
       "      <td>realdonaldtrump ozone is one the most powerfu...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.242881e+18</td>\n",
       "      <td>rt  occupyonelove   lillianorlando4  oveoblu  ...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.242893e+18</td>\n",
       "      <td>scottyddoogie  unperturbable  repadamschiff w...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1.242924e+18</td>\n",
       "      <td>but the virus and the disease it causes in hum...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1.242933e+18</td>\n",
       "      <td>w_terrence covid 19   chinese originated viru...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1.242931e+18</td>\n",
       "      <td>secpompeo this particular coronavirus has a n...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1.242697e+18</td>\n",
       "      <td>realdonaldtrump the novel strain of coronavir...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1.242705e+18</td>\n",
       "      <td>lilyella_tang  ptcwang  jlin7 official names ...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1.242816e+18</td>\n",
       "      <td>realdonaldtrump no  states governors don t ha...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1.242843e+18</td>\n",
       "      <td>for first time in modern history  the entire  ...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1.242955e+18</td>\n",
       "      <td>broncogto covid 19 is the name given by the w...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1.242708e+18</td>\n",
       "      <td>jarettsays  jasmynbeknowing as a scientist  i...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1.242783e+18</td>\n",
       "      <td>rt  mariankamensky1  hamsterweichei\\nbitte les...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1.242819e+18</td>\n",
       "      <td>intensive care doctor warns of  tsunami  of uk...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1.242840e+18</td>\n",
       "      <td>derlader  t2av1s  regalpinguino  eugenegu  re...</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1.243223e+18</td>\n",
       "      <td>rt  christinepolon1  covid 19  is an infectiou...</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>1.243195e+18</td>\n",
       "      <td>rt  christinepolon1  chloroquine phosphate is ...</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>1.243154e+18</td>\n",
       "      <td>secpompeo its coronavirus or sars cov 2   the...</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1.243105e+18</td>\n",
       "      <td>rt  christinepolon1  chloroquine phosphate is ...</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>1.243077e+18</td>\n",
       "      <td>rt  christinepolon1  covid 19  is an infectiou...</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1.243077e+18</td>\n",
       "      <td>rt  christinepolon1  chloroquine phosphate is ...</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1.243077e+18</td>\n",
       "      <td>rt  christinepolon1  covid 19  is an infectiou...</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>1.243063e+18</td>\n",
       "      <td>mattwolking and most people do not understand...</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>1.243054e+18</td>\n",
       "      <td>rt  tomandsteffani  remember when trump called...</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>1.243002e+18</td>\n",
       "      <td>nelliestirs no  they are  detecting   sars co...</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>1.242986e+18</td>\n",
       "      <td>  trump      covid19    ...</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1.243261e+18</td>\n",
       "      <td>rt  vinish_ind   _ankahi  alokg2k  neerangauta...</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>1.243260e+18</td>\n",
       "      <td>_ankahi  alokg2k  neerangautam  sangitarchopr...</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3354</th>\n",
       "      <td>5122</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  funder  cnn s jim acosta just called out t...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3355</th>\n",
       "      <td>5123</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  funder  cnn s jim acosta just called out t...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3356</th>\n",
       "      <td>5124</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  realjameswoods  trump shows off new rapid ...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3357</th>\n",
       "      <td>5125</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  funder  trump is letting leaders of corpor...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3358</th>\n",
       "      <td>5126</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  jesselehrich  illinois asked trump for mor...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3359</th>\n",
       "      <td>5127</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  asimplepatriot  we have our idiots in texa...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>5128</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  atrupar  here s dr fauci immediately pouri...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3361</th>\n",
       "      <td>5129</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  marshablackburn  nancy pelosi is accusing ...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>5130</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  waynedupreeshow  while democrat governors ...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>5131</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  realjameswoods  trump shows off new rapid ...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364</th>\n",
       "      <td>5132</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  forwardarc  so reprehensible  amp  upsetti...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365</th>\n",
       "      <td>5133</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  capaction  trump today   we re doing a gre...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366</th>\n",
       "      <td>5134</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  weinsteinlaw   nobody would have ever thou...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3367</th>\n",
       "      <td>5135</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  funder  everyone needs to stop airing trum...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3368</th>\n",
       "      <td>5136</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  realjameswoods  trump shows off new rapid ...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3369</th>\n",
       "      <td>5137</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  realjameswoods  trump shows off new rapid ...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3370</th>\n",
       "      <td>5138</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  eleconomista  ford y ge producirn ventila...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3371</th>\n",
       "      <td>5139</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  gregmolidor  breaking news \\n\\nillinois go...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3372</th>\n",
       "      <td>5140</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  marshablackburn  nancy pelosi is accusing ...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3373</th>\n",
       "      <td>5141</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  capaction  trump today   we re doing a gre...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3374</th>\n",
       "      <td>5142</td>\n",
       "      <td>1.244775e+18</td>\n",
       "      <td>rt  thejordanrachel  president trump   donates...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>5143</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  funder    cnn  amp   msnbc should stop air...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>5144</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  oliverdarcy  cnn s  acostareads trump his...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>5145</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  gregmolidor  a trump loving florida megach...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>5146</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>larryoconnor  realmikelindell no one is mad a...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>5147</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  inevitable_et  this is weird \\nhow an aust...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>5148</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  realjameswoods  trump shows off new rapid ...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>5149</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  stormisuponus  peak is behind us  remedy a...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3382</th>\n",
       "      <td>5150</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  funder    cnn  amp   msnbc should stop air...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383</th>\n",
       "      <td>5151</td>\n",
       "      <td>1.244770e+18</td>\n",
       "      <td>rt  trumptrain1111  mark cuban telling it like...</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3384 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index       tweetid                                               text  \\\n",
       "0         0  1.242949e+18   instinctnaturel  tjcrick4  dilenz2  rachs80  ...   \n",
       "1         1  1.242718e+18  rt  rebekahsright   the sars cov 2 coronavirus...   \n",
       "2         2  1.242828e+18   realdonaldtrump ozone is one the most powerfu...   \n",
       "3         3  1.242881e+18  rt  occupyonelove   lillianorlando4  oveoblu  ...   \n",
       "4         4  1.242893e+18   scottyddoogie  unperturbable  repadamschiff w...   \n",
       "5         5  1.242924e+18  but the virus and the disease it causes in hum...   \n",
       "6         6  1.242933e+18   w_terrence covid 19   chinese originated viru...   \n",
       "7         7  1.242931e+18   secpompeo this particular coronavirus has a n...   \n",
       "8         8  1.242697e+18   realdonaldtrump the novel strain of coronavir...   \n",
       "9         9  1.242705e+18   lilyella_tang  ptcwang  jlin7 official names ...   \n",
       "10       10  1.242816e+18   realdonaldtrump no  states governors don t ha...   \n",
       "11       11  1.242843e+18  for first time in modern history  the entire  ...   \n",
       "12       12  1.242955e+18   broncogto covid 19 is the name given by the w...   \n",
       "13       13  1.242708e+18   jarettsays  jasmynbeknowing as a scientist  i...   \n",
       "14       14  1.242783e+18  rt  mariankamensky1  hamsterweichei\\nbitte les...   \n",
       "15       15  1.242819e+18  intensive care doctor warns of  tsunami  of uk...   \n",
       "16       16  1.242840e+18   derlader  t2av1s  regalpinguino  eugenegu  re...   \n",
       "17       17  1.243223e+18  rt  christinepolon1  covid 19  is an infectiou...   \n",
       "18       18  1.243195e+18  rt  christinepolon1  chloroquine phosphate is ...   \n",
       "19       19  1.243154e+18   secpompeo its coronavirus or sars cov 2   the...   \n",
       "20       20  1.243105e+18  rt  christinepolon1  chloroquine phosphate is ...   \n",
       "21       21  1.243077e+18  rt  christinepolon1  covid 19  is an infectiou...   \n",
       "22       22  1.243077e+18  rt  christinepolon1  chloroquine phosphate is ...   \n",
       "23       23  1.243077e+18  rt  christinepolon1  covid 19  is an infectiou...   \n",
       "24       24  1.243063e+18   mattwolking and most people do not understand...   \n",
       "25       25  1.243054e+18  rt  tomandsteffani  remember when trump called...   \n",
       "26       26  1.243002e+18   nelliestirs no  they are  detecting   sars co...   \n",
       "27       27  1.242986e+18    trump      covid19    ...   \n",
       "28       29  1.243261e+18  rt  vinish_ind   _ankahi  alokg2k  neerangauta...   \n",
       "29       30  1.243260e+18   _ankahi  alokg2k  neerangautam  sangitarchopr...   \n",
       "...     ...           ...                                                ...   \n",
       "3354   5122  1.244770e+18  rt  funder  cnn s jim acosta just called out t...   \n",
       "3355   5123  1.244770e+18  rt  funder  cnn s jim acosta just called out t...   \n",
       "3356   5124  1.244770e+18  rt  realjameswoods  trump shows off new rapid ...   \n",
       "3357   5125  1.244770e+18  rt  funder  trump is letting leaders of corpor...   \n",
       "3358   5126  1.244770e+18  rt  jesselehrich  illinois asked trump for mor...   \n",
       "3359   5127  1.244770e+18  rt  asimplepatriot  we have our idiots in texa...   \n",
       "3360   5128  1.244770e+18  rt  atrupar  here s dr fauci immediately pouri...   \n",
       "3361   5129  1.244770e+18  rt  marshablackburn  nancy pelosi is accusing ...   \n",
       "3362   5130  1.244770e+18  rt  waynedupreeshow  while democrat governors ...   \n",
       "3363   5131  1.244770e+18  rt  realjameswoods  trump shows off new rapid ...   \n",
       "3364   5132  1.244770e+18  rt  forwardarc  so reprehensible  amp  upsetti...   \n",
       "3365   5133  1.244770e+18  rt  capaction  trump today   we re doing a gre...   \n",
       "3366   5134  1.244770e+18  rt  weinsteinlaw   nobody would have ever thou...   \n",
       "3367   5135  1.244770e+18  rt  funder  everyone needs to stop airing trum...   \n",
       "3368   5136  1.244770e+18  rt  realjameswoods  trump shows off new rapid ...   \n",
       "3369   5137  1.244770e+18  rt  realjameswoods  trump shows off new rapid ...   \n",
       "3370   5138  1.244770e+18  rt  eleconomista  ford y ge producirn ventila...   \n",
       "3371   5139  1.244770e+18  rt  gregmolidor  breaking news \\n\\nillinois go...   \n",
       "3372   5140  1.244770e+18  rt  marshablackburn  nancy pelosi is accusing ...   \n",
       "3373   5141  1.244770e+18  rt  capaction  trump today   we re doing a gre...   \n",
       "3374   5142  1.244775e+18  rt  thejordanrachel  president trump   donates...   \n",
       "3375   5143  1.244770e+18  rt  funder    cnn  amp   msnbc should stop air...   \n",
       "3376   5144  1.244770e+18  rt  oliverdarcy  cnn s  acostareads trump his...   \n",
       "3377   5145  1.244770e+18  rt  gregmolidor  a trump loving florida megach...   \n",
       "3378   5146  1.244770e+18   larryoconnor  realmikelindell no one is mad a...   \n",
       "3379   5147  1.244770e+18  rt  inevitable_et  this is weird \\nhow an aust...   \n",
       "3380   5148  1.244770e+18  rt  realjameswoods  trump shows off new rapid ...   \n",
       "3381   5149  1.244770e+18  rt  stormisuponus  peak is behind us  remedy a...   \n",
       "3382   5150  1.244770e+18  rt  funder    cnn  amp   msnbc should stop air...   \n",
       "3383   5151  1.244770e+18  rt  trumptrain1111  mark cuban telling it like...   \n",
       "\n",
       "               d   lb    r  Unnamed: 5  Unnamed: 6  \n",
       "0     2020-03-25  NaN  0.0         NaN         NaN  \n",
       "1     2020-03-25  4.0  1.0         NaN         NaN  \n",
       "2     2020-03-25  1.0  0.0         NaN         NaN  \n",
       "3     2020-03-25  4.0  0.0         NaN         NaN  \n",
       "4     2020-03-25  2.0  1.0         NaN         NaN  \n",
       "5     2020-03-25  2.0  1.0         NaN         NaN  \n",
       "6     2020-03-25  NaN  1.0         NaN         NaN  \n",
       "7     2020-03-25  5.0  0.0         NaN         NaN  \n",
       "8     2020-03-25  2.0  0.0         NaN         NaN  \n",
       "9     2020-03-25  5.0  1.0         NaN         NaN  \n",
       "10    2020-03-25  2.0  1.0         NaN         NaN  \n",
       "11    2020-03-25  4.0  1.0         NaN         NaN  \n",
       "12    2020-03-25  5.0  1.0         NaN         NaN  \n",
       "13    2020-03-25  2.0  0.0         NaN         NaN  \n",
       "14    2020-03-25  NaN  0.0         NaN         NaN  \n",
       "15    2020-03-25  4.0  0.0         NaN         NaN  \n",
       "16    2020-03-25  NaN  0.0         NaN         NaN  \n",
       "17    2020-03-26  NaN  0.0         NaN         NaN  \n",
       "18    2020-03-26  NaN  0.0         NaN         NaN  \n",
       "19    2020-03-26  4.0  1.0         NaN         NaN  \n",
       "20    2020-03-26  NaN  0.0         NaN         NaN  \n",
       "21    2020-03-26  3.0  1.0         NaN         NaN  \n",
       "22    2020-03-26  NaN  0.0         NaN         NaN  \n",
       "23    2020-03-26  NaN  0.0         NaN         NaN  \n",
       "24    2020-03-26  NaN  0.0         NaN         NaN  \n",
       "25    2020-03-26  2.0  1.0         NaN         NaN  \n",
       "26    2020-03-26  3.0  1.0         NaN         NaN  \n",
       "27    2020-03-26  NaN  0.0         NaN         NaN  \n",
       "28    2020-03-26  NaN  0.0         NaN         NaN  \n",
       "29    2020-03-26  NaN  0.0         NaN         NaN  \n",
       "...          ...  ...  ...         ...         ...  \n",
       "3354  2020-03-30  NaN  1.0         NaN         NaN  \n",
       "3355  2020-03-30  NaN  1.0         NaN         NaN  \n",
       "3356  2020-03-30  NaN  1.0         NaN         NaN  \n",
       "3357  2020-03-30  NaN  1.0         NaN         NaN  \n",
       "3358  2020-03-30  NaN  0.0         NaN         NaN  \n",
       "3359  2020-03-30  NaN  0.0         NaN         NaN  \n",
       "3360  2020-03-30  NaN  0.0         NaN         NaN  \n",
       "3361  2020-03-30  NaN  1.0         NaN         NaN  \n",
       "3362  2020-03-30  NaN  0.0         NaN         NaN  \n",
       "3363  2020-03-30  NaN  1.0         NaN         NaN  \n",
       "3364  2020-03-30  NaN  0.0         NaN         NaN  \n",
       "3365  2020-03-30  NaN  0.0         NaN         NaN  \n",
       "3366  2020-03-30  NaN  1.0         NaN         NaN  \n",
       "3367  2020-03-30  NaN  0.0         NaN         NaN  \n",
       "3368  2020-03-30  NaN  1.0         NaN         NaN  \n",
       "3369  2020-03-30  NaN  1.0         NaN         NaN  \n",
       "3370  2020-03-30  NaN  0.0         NaN         NaN  \n",
       "3371  2020-03-30  NaN  0.0         NaN         NaN  \n",
       "3372  2020-03-30  NaN  1.0         NaN         NaN  \n",
       "3373  2020-03-30  NaN  1.0         NaN         NaN  \n",
       "3374  2020-03-30  NaN  1.0         NaN         NaN  \n",
       "3375  2020-03-30  NaN  0.0         NaN         NaN  \n",
       "3376  2020-03-30  NaN  1.0         NaN         NaN  \n",
       "3377  2020-03-30  NaN  0.0         NaN         NaN  \n",
       "3378  2020-03-30  NaN  1.0         NaN         NaN  \n",
       "3379  2020-03-30  NaN  0.0         NaN         NaN  \n",
       "3380  2020-03-30  NaN  1.0         NaN         NaN  \n",
       "3381  2020-03-30  NaN  0.0         NaN         NaN  \n",
       "3382  2020-03-30  NaN  0.0         NaN         NaN  \n",
       "3383  2020-03-30  NaN  0.0         NaN         NaN  \n",
       "\n",
       "[3384 rows x 8 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit relevant data into count vector\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y_rel = lbl_enc.fit_transform(relevant_cleaned.r.values)\n",
    "X_train_rel, X_val_rel, y_train_rel, y_val_rel = train_test_split(relevant_cleaned.text.values, y_rel, stratify=y_rel, random_state=23, test_size=0.4, shuffle=True)\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word')\n",
    "count_vect.fit(relevant_cleaned['text'])\n",
    "X_train_rel_count =  count_vect.transform(X_train_rel)\n",
    "X_val_rel_count =  count_vect.transform(X_val_rel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes count vectors accuracy 0.5642540620384048\n",
      "lsvm using count vectors accuracy 0.5974889217134417\n",
      "log reg count vectors accuracy 0.5952732644017725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yindima/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/yindima/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest with count vectors accuracy 0.5989660265878878\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Multinomial Naive Bayes Classifier\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_rel_count, y_train_rel)\n",
    "y_pred = nb.predict(X_val_rel_count)\n",
    "print('naive bayes count vectors accuracy %s' % accuracy_score(y_pred, y_val_rel))\n",
    "\n",
    "# Model 2: Linear SVM\n",
    "\n",
    "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
    "lsvm.fit(X_train_rel_count, y_train_rel)\n",
    "y_pred = lsvm.predict(X_val_rel_count)\n",
    "print('lsvm using count vectors accuracy %s' % accuracy_score(y_pred, y_val_rel))\n",
    "\n",
    "# Model 3: Logistic Regression\n",
    "\n",
    "logreg = LogisticRegression(C=1)\n",
    "logreg.fit(X_train_rel_count, y_train_rel)\n",
    "y_pred = logreg.predict(X_val_rel_count)\n",
    "print('log reg count vectors accuracy %s' % accuracy_score(y_pred, y_val_rel))\n",
    "\n",
    "# Model 4: Random Forest Classifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(X_train_rel_count, y_train_rel)\n",
    "y_pred = rf.predict(X_val_rel_count)\n",
    "print('random forest with count vectors accuracy %s' % accuracy_score(y_pred, y_val_rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes tfidf accuracy 0.5480059084194978\n",
      "svm using tfidf accuracy 0.5753323485967504\n",
      "log reg tfidf accuracy 0.5672082717872969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yindima/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/yindima/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest tfidf accuracy 0.5576070901033974\n"
     ]
    }
   ],
   "source": [
    "#Try TF-IDF\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y_rel = lbl_enc.fit_transform(relevant_cleaned.r.values)\n",
    "X_train_rel, X_val_rel, y_train_rel, y_val_rel = train_test_split(relevant_cleaned.text.values, y_rel, stratify=y_rel, random_state=23, test_size=0.4, shuffle=True)\n",
    "tfidf = TfidfVectorizer(max_features=1000, analyzer='word',ngram_range=(1,3))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_rel)\n",
    "X_val_tfidf = tfidf.fit_transform(X_val_rel)\n",
    "\n",
    "# Model 1: Multinomial Naive Bayes Classifier\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train_rel)\n",
    "y_pred = nb.predict(X_val_tfidf)\n",
    "print(\"naive bayes tfidf accuracy %s\" %  accuracy_score(y_pred, y_val_rel))\n",
    "\n",
    "# Model 2: Linear SVM\n",
    "\n",
    "lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)\n",
    "lsvm.fit(X_train_tfidf, y_train_rel)\n",
    "y_pred = lsvm.predict(X_val_tfidf)\n",
    "print('svm using tfidf accuracy %s' % accuracy_score(y_pred, y_val_rel))\n",
    "\n",
    "# Model 3: logistic regression\n",
    "\n",
    "logreg = LogisticRegression(C=1)\n",
    "logreg.fit(X_train_tfidf, y_train_rel)\n",
    "y_pred = logreg.predict(X_val_tfidf)\n",
    "print('log reg tfidf accuracy %s' % accuracy_score(y_pred, y_val_rel))\n",
    "\n",
    "# Model 4: Random Forest Classifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(X_train_tfidf, y_train_rel)\n",
    "y_pred = rf.predict(X_val_tfidf)\n",
    "print('random forest tfidf accuracy %s' % accuracy_score(y_pred, y_val_rel))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
